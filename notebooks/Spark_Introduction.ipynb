{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import abspath\n",
    "from pyspark.sql import SparkSession, HiveContext\n",
    "import pyspark.sql.functions as F\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Session\n",
    "\n",
    "**Spark session** is a unified entry point for all spark applications starting from Spark 2.0. Instead of having a Spark context, Hive context, SQL context, now all of it is encapsulated in a Spark session.\n",
    "\n",
    "**Resources:**\n",
    " * [A tale of Spark Session and Spark Context](https://medium.com/@achilleus/spark-session-10d0d66d1d24)\n",
    "\n",
    "Create Spark session with `SparkSession.builder`:\n",
    " * `config(\"spark.sql.warehouse.dir\", warehouse_location)` - `warehouse_location` points to the default location for managed databases and tables\n",
    " * `config('spark.driver.extraJavaOptions','-Dderby.system.home=../data/tmp')` points where `metastore_db` and `derby.log` are created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warehouse_location = abspath('../data/spark-warehouse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "         .builder \\\n",
    "         .config(\"spark.sql.warehouse.dir\", warehouse_location) \\\n",
    "         .config('spark.driver.extraJavaOptions','-Dderby.system.home=../data/tmp') \\\n",
    "         .enableHiveSupport() \\\n",
    "         .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Spark Sessions\n",
    "\n",
    "Creating multiple Spark sessions can cause issues, so it's best practice to use the `getOrCreate()` method. It returns an existing Spark session if there's already one in the environment, or creates a new one if necessary. Let's test this and create another Spark session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_2 = (SparkSession.builder.enableHiveSupport().getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can verify that both Spark sessions are the same objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7f83a1d54a90>\n",
      "<pyspark.sql.session.SparkSession object at 0x7f83a1d54a90>\n"
     ]
    }
   ],
   "source": [
    "print(spark)\n",
    "print(spark_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Spark version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, Spark context (and other contexts) are accessible from the Spark session object - `spark`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://host.docker.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example: access Spark configuration parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.host', 'host.docker.internal'),\n",
       " ('spark.sql.catalogImplementation', 'hive'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.driver.port', '52422'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.id', 'local-1574741565100'),\n",
       " ('spark.driver.extraJavaOptions', '-Dderby.system.home=../data/tmp'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.app.name', 'pyspark-shell'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  '/mnt/d/pakhotin/Personal/Projects/Advanced-Spark/data/spark-warehouse')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext._conf.getAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read from a File to Spark Data Frame "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can read data in Spark data frames, for example, from a `csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris = spark.read.csv(\"../data/raw/iris.csv\", header=True, inferSchema =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, the data above is the famous _Iris_ sample by Fisher:\n",
    " * [_Iris_ Data Set at Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/iris)\n",
    " * [_Iris_ flower data set](https://en.wikipedia.org/wiki/Iris_flower_data_set)\n",
    " * [R. A. Fisher (1936). \"The use of multiple measurements in taxonomic problems\". Annals of Eugenics. 7 (2): 179â€“188](https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-1809.1936.tb02137.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[sepal_length_cm: double, sepal_width_cm: double, petal_length_cm: double, petal_width_cm: double, class_iris: string]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+---------------+--------------+-----------+\n",
      "|sepal_length_cm|sepal_width_cm|petal_length_cm|petal_width_cm| class_iris|\n",
      "+---------------+--------------+---------------+--------------+-----------+\n",
      "|            5.1|           3.5|            1.4|           0.2|Iris-setosa|\n",
      "|            4.9|           3.0|            1.4|           0.2|Iris-setosa|\n",
      "|            4.7|           3.2|            1.3|           0.2|Iris-setosa|\n",
      "|            4.6|           3.1|            1.5|           0.2|Iris-setosa|\n",
      "|            5.0|           3.6|            1.4|           0.2|Iris-setosa|\n",
      "+---------------+--------------+---------------+--------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_iris.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Spark Data Frame to Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the data frame `df_iris` to Hive table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Table `iris_tb` already exists.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/Ubnt_PySpark_py36/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/Ubnt_PySpark_py36/lib/python3.6/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o76.saveAsTable.\n: org.apache.spark.sql.AnalysisException: Table `iris_tb` already exists.;\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:415)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:400)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-1119a3d7f79f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_iris\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"iris_tb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/Ubnt_PySpark_py36/lib/python3.6/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msaveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 775\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/Ubnt_PySpark_py36/lib/python3.6/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/Ubnt_PySpark_py36/lib/python3.6/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Table `iris_tb` already exists.;'"
     ]
    }
   ],
   "source": [
    "df_iris.write.saveAsTable(\"iris_tb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see **tables** available in Spark cluster with `catalog.listTables()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Table(name='iris_tb', database='default', description=None, tableType='MANAGED', isTemporary=False)]\n"
     ]
    }
   ],
   "source": [
    "print(spark.catalog.listTables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see **databases** available in Spark cluster with `catalog.listDatabases()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Database(name='default', description='Default Hive database', locationUri='file:/mnt/d/pakhotin/Personal/Projects/Advanced-Spark/data/spark-warehouse')]\n"
     ]
    }
   ],
   "source": [
    "print(spark.catalog.listDatabases())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, it is located inside `spark-warehouse` which we defined above in Spark configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also register Spark data frame into **TEMPORARY** table to make it available within other contexts as well (for example, within SQL context) but only from the specific Spark session that was used to create the data frame.\n",
    "\n",
    "There are two methods:\n",
    " * `createTempView()` - the lifetime of this temporary table is tied to the `SparkSession` that was used to create this `DataFrame`. It throws `TempTableAlreadyExistsException`, if the view name already exists in the catalog.\n",
    " * `createOrReplaceTempView()` -  similar to above but safely creates a new temporary table if nothing was there before, or updates an existing table if one was already defined. This is recommended method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris.createOrReplaceTempView(\"iris_temp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine catalog again and see that new table `iris_temp` is there and listed as temporary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Table(name='iris_tb', database='default', description=None, tableType='MANAGED', isTemporary=False), Table(name='iris_temp', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]\n"
     ]
    }
   ],
   "source": [
    "print(spark.catalog.listTables())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read from Table to Spark Data Frame "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can read the **entire** table into data frame using method `table()` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris_2 = spark.table(\"iris_tb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+---------------+--------------+-----------+\n",
      "|sepal_length_cm|sepal_width_cm|petal_length_cm|petal_width_cm| class_iris|\n",
      "+---------------+--------------+---------------+--------------+-----------+\n",
      "|            5.1|           3.5|            1.4|           0.2|Iris-setosa|\n",
      "|            4.9|           3.0|            1.4|           0.2|Iris-setosa|\n",
      "|            4.7|           3.2|            1.3|           0.2|Iris-setosa|\n",
      "|            4.6|           3.1|            1.5|           0.2|Iris-setosa|\n",
      "|            5.0|           3.6|            1.4|           0.2|Iris-setosa|\n",
      "|            5.4|           3.9|            1.7|           0.4|Iris-setosa|\n",
      "|            4.6|           3.4|            1.4|           0.3|Iris-setosa|\n",
      "|            5.0|           3.4|            1.5|           0.2|Iris-setosa|\n",
      "|            4.4|           2.9|            1.4|           0.2|Iris-setosa|\n",
      "|            4.9|           3.1|            1.5|           0.1|Iris-setosa|\n",
      "+---------------+--------------+---------------+--------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_iris_2.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can perform **SQL query** on the table to read results into data frame. Method `sql()` allows to run queries as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"FROM iris_tb SELECT * WHERE class_iris = 'Iris-versicolor' LIMIT 10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowers10 = spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+---------------+--------------+---------------+\n",
      "|sepal_length_cm|sepal_width_cm|petal_length_cm|petal_width_cm|     class_iris|\n",
      "+---------------+--------------+---------------+--------------+---------------+\n",
      "|            7.0|           3.2|            4.7|           1.4|Iris-versicolor|\n",
      "|            6.4|           3.2|            4.5|           1.5|Iris-versicolor|\n",
      "|            6.9|           3.1|            4.9|           1.5|Iris-versicolor|\n",
      "|            5.5|           2.3|            4.0|           1.3|Iris-versicolor|\n",
      "|            6.5|           2.8|            4.6|           1.5|Iris-versicolor|\n",
      "|            5.7|           2.8|            4.5|           1.3|Iris-versicolor|\n",
      "|            6.3|           3.3|            4.7|           1.6|Iris-versicolor|\n",
      "|            4.9|           2.4|            3.3|           1.0|Iris-versicolor|\n",
      "|            6.6|           2.9|            4.6|           1.3|Iris-versicolor|\n",
      "|            5.2|           2.7|            3.9|           1.4|Iris-versicolor|\n",
      "+---------------+--------------+---------------+--------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flowers10.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Spark Data Frame to Pandas Data Frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If resulting Spark data frame has manageable size it could be converted to **Pandas** dataframe with method `toPandas()` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_flowers10 = flowers10.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Data columns (total 5 columns):\n",
      "sepal_length_cm    10 non-null float64\n",
      "sepal_width_cm     10 non-null float64\n",
      "petal_length_cm    10 non-null float64\n",
      "petal_width_cm     10 non-null float64\n",
      "class_iris         10 non-null object\n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 480.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "pdf_flowers10.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length_cm</th>\n",
       "      <th>sepal_width_cm</th>\n",
       "      <th>petal_length_cm</th>\n",
       "      <th>petal_width_cm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.100000</td>\n",
       "      <td>2.870000</td>\n",
       "      <td>4.370000</td>\n",
       "      <td>1.380000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.727247</td>\n",
       "      <td>0.340098</td>\n",
       "      <td>0.487739</td>\n",
       "      <td>0.168655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.900000</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.550000</td>\n",
       "      <td>2.725000</td>\n",
       "      <td>4.125000</td>\n",
       "      <td>1.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.350000</td>\n",
       "      <td>2.850000</td>\n",
       "      <td>4.550000</td>\n",
       "      <td>1.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.575000</td>\n",
       "      <td>3.175000</td>\n",
       "      <td>4.675000</td>\n",
       "      <td>1.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>4.900000</td>\n",
       "      <td>1.600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sepal_length_cm  sepal_width_cm  petal_length_cm  petal_width_cm\n",
       "count        10.000000       10.000000        10.000000       10.000000\n",
       "mean          6.100000        2.870000         4.370000        1.380000\n",
       "std           0.727247        0.340098         0.487739        0.168655\n",
       "min           4.900000        2.300000         3.300000        1.000000\n",
       "25%           5.550000        2.725000         4.125000        1.300000\n",
       "50%           6.350000        2.850000         4.550000        1.400000\n",
       "75%           6.575000        3.175000         4.675000        1.500000\n",
       "max           7.000000        3.300000         4.900000        1.600000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_flowers10.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Pandas Data Frame to Spark Data Frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert Pandas data frame to Spark data frame using `createDataFrame()` method with Pandas data frame as argument as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "flowers10_tmp = spark.createDataFrame(pdf_flowers10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[sepal_length_cm: double, sepal_width_cm: double, petal_length_cm: double, petal_width_cm: double, class_iris: string]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flowers10_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+---------------+--------------+---------------+\n",
      "|sepal_length_cm|sepal_width_cm|petal_length_cm|petal_width_cm|     class_iris|\n",
      "+---------------+--------------+---------------+--------------+---------------+\n",
      "|            7.0|           3.2|            4.7|           1.4|Iris-versicolor|\n",
      "|            6.4|           3.2|            4.5|           1.5|Iris-versicolor|\n",
      "|            6.9|           3.1|            4.9|           1.5|Iris-versicolor|\n",
      "|            5.5|           2.3|            4.0|           1.3|Iris-versicolor|\n",
      "|            6.5|           2.8|            4.6|           1.5|Iris-versicolor|\n",
      "|            5.7|           2.8|            4.5|           1.3|Iris-versicolor|\n",
      "|            6.3|           3.3|            4.7|           1.6|Iris-versicolor|\n",
      "|            4.9|           2.4|            3.3|           1.0|Iris-versicolor|\n",
      "|            6.6|           2.9|            4.6|           1.3|Iris-versicolor|\n",
      "|            5.2|           2.7|            3.9|           1.4|Iris-versicolor|\n",
      "+---------------+--------------+---------------+--------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flowers10_tmp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating Data in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a New Column in a Data Frame\n",
    "\n",
    "Method `withColumn()` allows to perform column-wise operations. It takes two arguments:\n",
    " * `colName` - a string containing the name of the new column\n",
    " * `col` - a column expression\n",
    "and returns a new DataFrame with the new column added.  Note, data frames in Spark are **imutable**, i.e. can't be changed in place, but we can reassign resulting data frame to the initial data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris = df_iris.withColumn(\"sepal_area_cm2\", df_iris.sepal_length_cm * df_iris.sepal_width_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+---------------+--------------+-----------+------------------+\n",
      "|sepal_length_cm|sepal_width_cm|petal_length_cm|petal_width_cm| class_iris|    sepal_area_cm2|\n",
      "+---------------+--------------+---------------+--------------+-----------+------------------+\n",
      "|            5.1|           3.5|            1.4|           0.2|Iris-setosa|17.849999999999998|\n",
      "|            4.9|           3.0|            1.4|           0.2|Iris-setosa|14.700000000000001|\n",
      "|            4.7|           3.2|            1.3|           0.2|Iris-setosa|15.040000000000001|\n",
      "|            4.6|           3.1|            1.5|           0.2|Iris-setosa|             14.26|\n",
      "|            5.0|           3.6|            1.4|           0.2|Iris-setosa|              18.0|\n",
      "|            5.4|           3.9|            1.7|           0.4|Iris-setosa|21.060000000000002|\n",
      "|            4.6|           3.4|            1.4|           0.3|Iris-setosa|15.639999999999999|\n",
      "|            5.0|           3.4|            1.5|           0.2|Iris-setosa|              17.0|\n",
      "|            4.4|           2.9|            1.4|           0.2|Iris-setosa|             12.76|\n",
      "|            4.9|           3.1|            1.5|           0.1|Iris-setosa|15.190000000000001|\n",
      "+---------------+--------------+---------------+--------------+-----------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_iris.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can create a new column of boolean values based on a condition as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris = df_iris.withColumn(\"sepal_length_big\", df_iris.sepal_length_cm > 6.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+---------------+--------------+-----------+------------------+----------------+\n",
      "|sepal_length_cm|sepal_width_cm|petal_length_cm|petal_width_cm| class_iris|    sepal_area_cm2|sepal_length_big|\n",
      "+---------------+--------------+---------------+--------------+-----------+------------------+----------------+\n",
      "|            5.1|           3.5|            1.4|           0.2|Iris-setosa|17.849999999999998|           false|\n",
      "|            4.9|           3.0|            1.4|           0.2|Iris-setosa|14.700000000000001|           false|\n",
      "|            4.7|           3.2|            1.3|           0.2|Iris-setosa|15.040000000000001|           false|\n",
      "|            4.6|           3.1|            1.5|           0.2|Iris-setosa|             14.26|           false|\n",
      "|            5.0|           3.6|            1.4|           0.2|Iris-setosa|              18.0|           false|\n",
      "|            5.4|           3.9|            1.7|           0.4|Iris-setosa|21.060000000000002|           false|\n",
      "|            4.6|           3.4|            1.4|           0.3|Iris-setosa|15.639999999999999|           false|\n",
      "|            5.0|           3.4|            1.5|           0.2|Iris-setosa|              17.0|           false|\n",
      "|            4.4|           2.9|            1.4|           0.2|Iris-setosa|             12.76|           false|\n",
      "|            4.9|           3.1|            1.5|           0.1|Iris-setosa|15.190000000000001|           false|\n",
      "+---------------+--------------+---------------+--------------+-----------+------------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_iris.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming a Column in a Data Frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A column could be renamed using `withColumnRenamed()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris = df_iris.withColumnRenamed(\"sepal_area_cm2\", \"sepal_area_cm_squared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+---------------+--------------+-----------+---------------------+----------------+\n",
      "|sepal_length_cm|sepal_width_cm|petal_length_cm|petal_width_cm| class_iris|sepal_area_cm_squared|sepal_length_big|\n",
      "+---------------+--------------+---------------+--------------+-----------+---------------------+----------------+\n",
      "|            5.1|           3.5|            1.4|           0.2|Iris-setosa|   17.849999999999998|           false|\n",
      "|            4.9|           3.0|            1.4|           0.2|Iris-setosa|   14.700000000000001|           false|\n",
      "|            4.7|           3.2|            1.3|           0.2|Iris-setosa|   15.040000000000001|           false|\n",
      "|            4.6|           3.1|            1.5|           0.2|Iris-setosa|                14.26|           false|\n",
      "|            5.0|           3.6|            1.4|           0.2|Iris-setosa|                 18.0|           false|\n",
      "+---------------+--------------+---------------+--------------+-----------+---------------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_iris.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering Records in a Data Frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method `filter()` allows to select only **rows** from a Spark data frame that satisfy given condition. This method accpets one argument - the condition expression which could be constructed as  follows:\n",
    " * SQL expression\n",
    " * or Spark column of boolean values. \n",
    "\n",
    "The following is an example of SQL expression used for filtering. Note, that the expression must be **string** and doesn't contain data frame name (use `\"sepal_length_cm > 6.0\"` but not `\"df_iris.sepal_length_cm > 6.0\"`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+---------------+--------------+---------------+---------------------+----------------+\n",
      "|sepal_length_cm|sepal_width_cm|petal_length_cm|petal_width_cm|     class_iris|sepal_area_cm_squared|sepal_length_big|\n",
      "+---------------+--------------+---------------+--------------+---------------+---------------------+----------------+\n",
      "|            7.0|           3.2|            4.7|           1.4|Iris-versicolor|   22.400000000000002|            true|\n",
      "|            6.4|           3.2|            4.5|           1.5|Iris-versicolor|   20.480000000000004|            true|\n",
      "|            6.9|           3.1|            4.9|           1.5|Iris-versicolor|                21.39|            true|\n",
      "|            6.5|           2.8|            4.6|           1.5|Iris-versicolor|                 18.2|            true|\n",
      "|            6.3|           3.3|            4.7|           1.6|Iris-versicolor|                20.79|            true|\n",
      "|            6.6|           2.9|            4.6|           1.3|Iris-versicolor|   19.139999999999997|            true|\n",
      "|            6.1|           2.9|            4.7|           1.4|Iris-versicolor|   17.689999999999998|            true|\n",
      "|            6.7|           3.1|            4.4|           1.4|Iris-versicolor|                20.77|            true|\n",
      "|            6.2|           2.2|            4.5|           1.5|Iris-versicolor|   13.640000000000002|            true|\n",
      "|            6.1|           2.8|            4.0|           1.3|Iris-versicolor|                17.08|            true|\n",
      "+---------------+--------------+---------------+--------------+---------------+---------------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_iris.filter(\"sepal_length_cm > 6.0\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of Spark column of boolean values used for filtering. Note, it does contain name of the data frame (use `df_iris.sepal_length_cm > 6.0` but not `sepal_length_cm > 6.0`) and it is **not** string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+---------------+--------------+---------------+---------------------+----------------+\n",
      "|sepal_length_cm|sepal_width_cm|petal_length_cm|petal_width_cm|     class_iris|sepal_area_cm_squared|sepal_length_big|\n",
      "+---------------+--------------+---------------+--------------+---------------+---------------------+----------------+\n",
      "|            7.0|           3.2|            4.7|           1.4|Iris-versicolor|   22.400000000000002|            true|\n",
      "|            6.4|           3.2|            4.5|           1.5|Iris-versicolor|   20.480000000000004|            true|\n",
      "|            6.9|           3.1|            4.9|           1.5|Iris-versicolor|                21.39|            true|\n",
      "|            6.5|           2.8|            4.6|           1.5|Iris-versicolor|                 18.2|            true|\n",
      "|            6.3|           3.3|            4.7|           1.6|Iris-versicolor|                20.79|            true|\n",
      "|            6.6|           2.9|            4.6|           1.3|Iris-versicolor|   19.139999999999997|            true|\n",
      "|            6.1|           2.9|            4.7|           1.4|Iris-versicolor|   17.689999999999998|            true|\n",
      "|            6.7|           3.1|            4.4|           1.4|Iris-versicolor|                20.77|            true|\n",
      "|            6.2|           2.2|            4.5|           1.5|Iris-versicolor|   13.640000000000002|            true|\n",
      "|            6.1|           2.8|            4.0|           1.3|Iris-versicolor|                17.08|            true|\n",
      "+---------------+--------------+---------------+--------------+---------------+---------------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_iris.filter(df_iris.sepal_length_cm > 6.0).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that Spark column used in the filter could be defined separately as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_long_sepal = df_iris.sepal_length_cm > 6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+---------------+--------------+---------------+---------------------+----------------+\n",
      "|sepal_length_cm|sepal_width_cm|petal_length_cm|petal_width_cm|     class_iris|sepal_area_cm_squared|sepal_length_big|\n",
      "+---------------+--------------+---------------+--------------+---------------+---------------------+----------------+\n",
      "|            7.0|           3.2|            4.7|           1.4|Iris-versicolor|   22.400000000000002|            true|\n",
      "|            6.4|           3.2|            4.5|           1.5|Iris-versicolor|   20.480000000000004|            true|\n",
      "|            6.9|           3.1|            4.9|           1.5|Iris-versicolor|                21.39|            true|\n",
      "|            6.5|           2.8|            4.6|           1.5|Iris-versicolor|                 18.2|            true|\n",
      "|            6.3|           3.3|            4.7|           1.6|Iris-versicolor|                20.79|            true|\n",
      "|            6.6|           2.9|            4.6|           1.3|Iris-versicolor|   19.139999999999997|            true|\n",
      "|            6.1|           2.9|            4.7|           1.4|Iris-versicolor|   17.689999999999998|            true|\n",
      "|            6.7|           3.1|            4.4|           1.4|Iris-versicolor|                20.77|            true|\n",
      "|            6.2|           2.2|            4.5|           1.5|Iris-versicolor|   13.640000000000002|            true|\n",
      "|            6.1|           2.8|            4.0|           1.3|Iris-versicolor|                17.08|            true|\n",
      "+---------------+--------------+---------------+--------------+---------------+---------------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_iris.filter(filter_long_sepal).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Columns from a Data Frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method `select()` allows to select **columns** from Spark data frame with given names. This method accpets multiple arguments - names of columns as follows:\n",
    " * string name\n",
    " * or column object. \n",
    "\n",
    "The following is an example of **string** column name used for selection (use `\"sepal_length_cm\"` but not `\"df_iris.sepal_length_cm\"`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+\n",
      "|sepal_length_cm|sepal_width_cm|\n",
      "+---------------+--------------+\n",
      "|            5.1|           3.5|\n",
      "|            4.9|           3.0|\n",
      "|            4.7|           3.2|\n",
      "|            4.6|           3.1|\n",
      "|            5.0|           3.6|\n",
      "+---------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_iris.select(\"sepal_length_cm\", \"sepal_width_cm\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is an example of **column object** used for selection (use `df_iris.sepal_length_cm` but not `sepal_length_cm`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+\n",
      "|sepal_length_cm|sepal_width_cm|\n",
      "+---------------+--------------+\n",
      "|            5.1|           3.5|\n",
      "|            4.9|           3.0|\n",
      "|            4.7|           3.2|\n",
      "|            4.6|           3.1|\n",
      "|            5.0|           3.6|\n",
      "+---------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_iris.select(df_iris.sepal_length_cm, df_iris.sepal_width_cm).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can mix both types of arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+\n",
      "|sepal_length_cm|sepal_width_cm|\n",
      "+---------------+--------------+\n",
      "|            5.1|           3.5|\n",
      "|            4.9|           3.0|\n",
      "|            4.7|           3.2|\n",
      "|            4.6|           3.1|\n",
      "|            5.0|           3.6|\n",
      "+---------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_iris.select(\"sepal_length_cm\", df_iris.sepal_width_cm).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same method `select()` could be also used to apply **column-wise operations**. It is applied **only** to column objects as follows (applying to SQL strings `select(\"sepal_length_cm*10\")` would **not** work):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+---------------------+\n",
      "|(sepal_length_cm * 10)|(sepal_width_cm * 10)|\n",
      "+----------------------+---------------------+\n",
      "|                  51.0|                 35.0|\n",
      "|                  49.0|                 30.0|\n",
      "|                  47.0|                 32.0|\n",
      "|                  46.0|                 31.0|\n",
      "|                  50.0|                 36.0|\n",
      "+----------------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_iris.select(df_iris.sepal_length_cm*10, df_iris.sepal_width_cm*10).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we can use method `alias()` to rename selected and changed columns as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+\n",
      "|sepal_length_mm|sepal_width_mm|\n",
      "+---------------+--------------+\n",
      "|           51.0|          35.0|\n",
      "|           49.0|          30.0|\n",
      "|           47.0|          32.0|\n",
      "|           46.0|          31.0|\n",
      "|           50.0|          36.0|\n",
      "+---------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_iris.select( (df_iris.sepal_length_cm*10).alias(\"sepal_length_mm\"), (df_iris.sepal_width_cm*10).alias(\"sepal_width_mm\") ).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conviniently arguments of the `select()` method could be defined separately and then plugged in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+\n",
      "|sepal_length_mm|sepal_width_mm|\n",
      "+---------------+--------------+\n",
      "|           51.0|          35.0|\n",
      "|           49.0|          30.0|\n",
      "|           47.0|          32.0|\n",
      "|           46.0|          31.0|\n",
      "|           50.0|          36.0|\n",
      "+---------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sepal_length_mm = (df_iris.sepal_length_cm*10).alias(\"sepal_length_mm\")\n",
    "sepal_width_mm = (df_iris.sepal_width_cm*10).alias(\"sepal_width_mm\")\n",
    "df_iris.select(sepal_length_mm, sepal_width_mm).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, in the code above we use `df_iris.select(sepal_length_mm, sepal_width_mm)` but not `df_iris.select(df_iris.sepal_length_mm, df_iris.sepal_width_mm)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted above, selection **and** operation on columns with `select()` method could be performed using column objects only. But if we want to use SQL strings, then we have to use method `selectExpr()` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+---------------------+\n",
      "|(sepal_length_cm * 10)|(sepal_width_cm * 10)|\n",
      "+----------------------+---------------------+\n",
      "|                  51.0|                 35.0|\n",
      "|                  49.0|                 30.0|\n",
      "|                  47.0|                 32.0|\n",
      "|                  46.0|                 31.0|\n",
      "|                  50.0|                 36.0|\n",
      "+----------------------+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_iris.selectExpr(\"sepal_length_cm*10\", \"sepal_width_cm*10\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can rename new columns with SQL operator `AS` (similarly as we did above with `alias()` method):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+\n",
      "|sepal_length_mm|sepal_width_mm|\n",
      "+---------------+--------------+\n",
      "|           51.0|          35.0|\n",
      "|           49.0|          30.0|\n",
      "|           47.0|          32.0|\n",
      "|           46.0|          31.0|\n",
      "|           50.0|          36.0|\n",
      "+---------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_iris.selectExpr(\"sepal_length_cm*10 AS sepal_length_mm\", \"sepal_width_cm*10 sepal_width_mm\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's use both methods next to each other to demonstrate that results are the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+\n",
      "|sepal_length_mm|sepal_width_mm|\n",
      "+---------------+--------------+\n",
      "|           51.0|          35.0|\n",
      "|           49.0|          30.0|\n",
      "|           47.0|          32.0|\n",
      "|           46.0|          31.0|\n",
      "|           50.0|          36.0|\n",
      "+---------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_iris.select( (df_iris.sepal_length_cm*10).alias(\"sepal_length_mm\"), (df_iris.sepal_width_cm*10).alias(\"sepal_width_mm\") ).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+\n",
      "|sepal_length_mm|sepal_width_mm|\n",
      "+---------------+--------------+\n",
      "|           51.0|          35.0|\n",
      "|           49.0|          30.0|\n",
      "|           47.0|          32.0|\n",
      "|           46.0|          31.0|\n",
      "|           50.0|          36.0|\n",
      "+---------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_iris.selectExpr(\"sepal_length_cm*10 AS sepal_length_mm\", \"sepal_width_cm*10 sepal_width_mm\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difference between `withColumn` and `select`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method `select()` creates a new data frame with only columns specified as its arguments.\n",
    "\n",
    "Method `withColumn()` creates a new data fram with **all** columns of original data frame plus new column specified with its two arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|sepal_length_mm|\n",
      "+---------------+\n",
      "|           51.0|\n",
      "|           49.0|\n",
      "|           47.0|\n",
      "|           46.0|\n",
      "|           50.0|\n",
      "+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_iris.select( (df_iris.sepal_length_cm*10).alias(\"sepal_length_mm\") ).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+---------------+--------------+-----------+---------------------+----------------+---------------+\n",
      "|sepal_length_cm|sepal_width_cm|petal_length_cm|petal_width_cm| class_iris|sepal_area_cm_squared|sepal_length_big|sepal_length_mm|\n",
      "+---------------+--------------+---------------+--------------+-----------+---------------------+----------------+---------------+\n",
      "|            5.1|           3.5|            1.4|           0.2|Iris-setosa|   17.849999999999998|           false|           51.0|\n",
      "|            4.9|           3.0|            1.4|           0.2|Iris-setosa|   14.700000000000001|           false|           49.0|\n",
      "|            4.7|           3.2|            1.3|           0.2|Iris-setosa|   15.040000000000001|           false|           47.0|\n",
      "|            4.6|           3.1|            1.5|           0.2|Iris-setosa|                14.26|           false|           46.0|\n",
      "|            5.0|           3.6|            1.4|           0.2|Iris-setosa|                 18.0|           false|           50.0|\n",
      "+---------------+--------------+---------------+--------------+-----------+---------------------+----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_iris.withColumn(\"sepal_length_mm\", df_iris.sepal_length_cm * 10.0).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating Records in a Data Frame\n",
    "\n",
    "Aggregation methods follow `groupBy()` method wich creates a `GroupedData` object from Spark data frame.\n",
    "\n",
    "List of some aggregation methods:\n",
    " * `min()`\n",
    " * `max()`\n",
    " * `avg()`\n",
    " * `sum()`\n",
    " * `count()`\n",
    "\n",
    "Aggregation methods are used with **string** column names (column objects don't work, for example use `min(\"sepal_length_cm\")` but not `min(df_iris.sepal_length_cm)`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimal sepal length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|min(sepal_length_cm)|\n",
      "+--------------------+\n",
      "|                 4.3|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_iris.groupBy().min(\"sepal_length_cm\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximal sepal length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|max(sepal_length_cm)|\n",
      "+--------------------+\n",
      "|                 7.9|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_iris.groupBy().max(\"sepal_length_cm\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average sepal length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|avg(sepal_length_cm)|\n",
      "+--------------------+\n",
      "|   5.843333333333335|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_iris.groupBy().avg(\"sepal_length_cm\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sum of all sepal lengths in the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|sum(sepal_length_cm)|\n",
      "+--------------------+\n",
      "|   876.5000000000002|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_iris.groupBy().sum(\"sepal_length_cm\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counts of records in the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "|  150|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_iris.groupBy().count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `groupBy()` method could accept name of one or more columns as an argument. For example, we can group records by *Iris* classes (there are 3 of them *virginica*, *setosa* and *versicolor*) and calcualte average in each class separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+\n",
      "|     class_iris|avg(sepal_length_cm)|\n",
      "+---------------+--------------------+\n",
      "| Iris-virginica|   6.587999999999998|\n",
      "|    Iris-setosa|   5.005999999999999|\n",
      "|Iris-versicolor|               5.936|\n",
      "+---------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_iris.groupBy(\"class_iris\").avg(\"sepal_length_cm\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arguments to `groupBy()` could be column name strings (as `\"class_iris\"` used above) or column objects `df_iris.class_iris` as used in the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+\n",
      "|     class_iris|avg(sepal_length_cm)|\n",
      "+---------------+--------------------+\n",
      "| Iris-virginica|   6.587999999999998|\n",
      "|    Iris-setosa|   5.005999999999999|\n",
      "|Iris-versicolor|               5.936|\n",
      "+---------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_iris.groupBy(df_iris.class_iris).avg(\"sepal_length_cm\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another iinteresting application of `groupBy` method is together with `count` method to return number of records for each `Iris` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|     class_iris|count|\n",
      "+---------------+-----+\n",
      "| Iris-virginica|   50|\n",
      "|    Iris-setosa|   50|\n",
      "|Iris-versicolor|   50|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_iris.groupBy(df_iris.class_iris).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, any aggreagte function from `pyspark.sql.functions` module could be used with `groupBy()` and `agg()` methods. For example, let's group by *Iris* class and calculate standard deviation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------------------+\n",
      "|     class_iris|stddev_samp(sepal_length_cm)|\n",
      "+---------------+----------------------------+\n",
      "| Iris-virginica|           0.635879593274432|\n",
      "|    Iris-setosa|          0.3524896872134513|\n",
      "|Iris-versicolor|          0.5161711470638635|\n",
      "+---------------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_iris.groupBy(\"class_iris\").agg( F.stddev(\"sepal_length_cm\") ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, that we import `pyspark.sql.functions` as `F` at the beginning of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining Data Frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a data frame from `iris_class.csv` file that contains *Iris* classes and corresponding English names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris_class = spark.read.csv(\"../data/raw/iris_class.csv\", header=True, inferSchema =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+\n",
      "|     class_iris|           name_iris|\n",
      "+---------------+--------------------+\n",
      "|    Iris-setosa|Bristle-pointed iris|\n",
      "|Iris-versicolor|       Virginia iris|\n",
      "| Iris-virginica|Northern blue fla...|\n",
      "+---------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_iris_class.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method `join()` creates a new data frame combining information from 2 data frames using a column as a key. The method runs on 1st data frame and accepts 3 arguments:\n",
    " * 2nd data frame\n",
    " * `on` - name of column to join over (it should be the same name in both data frames; use `withColumnRenamed` if needed to rename)\n",
    " * `how` - defines different types of join, we use `leftouter` in the example below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+--------------+---------------+--------------+---------------------+----------------+--------------------+\n",
      "| class_iris|sepal_length_cm|sepal_width_cm|petal_length_cm|petal_width_cm|sepal_area_cm_squared|sepal_length_big|           name_iris|\n",
      "+-----------+---------------+--------------+---------------+--------------+---------------------+----------------+--------------------+\n",
      "|Iris-setosa|            5.1|           3.5|            1.4|           0.2|   17.849999999999998|           false|Bristle-pointed iris|\n",
      "|Iris-setosa|            4.9|           3.0|            1.4|           0.2|   14.700000000000001|           false|Bristle-pointed iris|\n",
      "|Iris-setosa|            4.7|           3.2|            1.3|           0.2|   15.040000000000001|           false|Bristle-pointed iris|\n",
      "|Iris-setosa|            4.6|           3.1|            1.5|           0.2|                14.26|           false|Bristle-pointed iris|\n",
      "|Iris-setosa|            5.0|           3.6|            1.4|           0.2|                 18.0|           false|Bristle-pointed iris|\n",
      "+-----------+---------------+--------------+---------------+--------------+---------------------+----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_iris.join(df_iris_class, on = \"class_iris\", how = \"leftouter\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
